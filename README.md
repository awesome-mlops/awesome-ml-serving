# awesome-ml-serving
A curated list of awesome open source and commercial platforms for serving models in production ðŸš€ 

* [Banana](https://banana.dev): Host your ML inference code on serverless GPUs and integrate it into your app with one line of code.
* [BentoML](https://github.com/bentoml/BentoML): Open-source platform for high-performance ML model serving.
* [BudgetML](https://github.com/ebhy/budgetml): Deploy a ML inference service on a budget in less than 10 lines of code.
* [Cortex](https://www.cortex.dev/): Machine learning model serving infrastructure.
* [Gradio](https://github.com/gradio-app/gradio): Create customizable UI components around your models.
* [GraphPipe](https://oracle.github.io/graphpipe): Machine learning model deployment made simple.
* [Hydrosphere](https://github.com/Hydrospheredata/hydro-serving): Platform for deploying your Machine Learning to production.
* [KFServe](https://github.com/kserve/kserve): Kubernetes custom resource definition for serving ML models on arbitrary frameworks.
* [Merlin](https://github.com/gojek/merlin): A platform for deploying and serving machine learning models.
* [Opyrator](https://github.com/ml-tooling/opyrator): Turns your ML code into microservices with web API, interactive GUI, and more.
* [PredictionIO](https://github.com/apache/predictionio): Event collection, deployment of algorithms, evaluation, querying predictive results via APIs.
* [Rune](https://github.com/hotg-ai/rune): Provides containers to encapsulate and deploy EdgeML pipelines and applications.
* [Seldon](https://www.seldon.io/): Take your ML projects from POC to production with maximum efficiency and minimal risk.
* [Streamlit](https://github.com/streamlit/streamlit): Lets you create apps for your ML projects with deceptively simple Python scripts.
* [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving): Flexible, high-performance serving system for ML models, designed for production.
* [TorchServe](https://github.com/pytorch/serve): A flexible and easy to use tool for serving PyTorch models.
* [Triton Inference Server](https://github.com/triton-inference-server/server): Provides an optimized cloud and edge inferencing solution.
